{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data/dataset/training_set'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-00238355097f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_data_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrescale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhorizontal_flip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshear_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzoom_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtraining_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow_from_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data/dataset/training_set'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_mode\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'binary'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation)\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0my\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m     \"\"\"\n\u001b[0;32m--> 943\u001b[0;31m     return DirectoryIterator(\n\u001b[0m\u001b[1;32m    944\u001b[0m         \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, dtype)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dtype'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m     super(DirectoryIterator, self).__init__(\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_data_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/keras_preprocessing/image/directory_iterator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, dtype)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data/dataset/training_set'"
     ]
    }
   ],
   "source": [
    "train_data_gen = ImageDataGenerator(rescale=1./255, horizontal_flip=True, shear_range=0.25, zoom_range=0.25)\n",
    "training_set = train_data_gen.flow_from_directory('Data/dataset/training_set', target_size=(64,64), class_mode ='binary', batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_data_gen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "testing_set = test_data_gen.flow_from_directory('Data/dataset/test_set', target_size=(64,64), class_mode='binary', batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#convolutional layer\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))\n",
    "cnn.add(tf.keras.layers.MaxPooling2D(pool_size=2, strides=2))\n",
    "\n",
    "cnn.add(tf.keras.layers.Conv2D(32, activation='relu', kernel_size=3))\n",
    "cnn.add(tf.keras.layers.MaxPooling2D(pool_size=2, strides=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flattening \n",
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full connection with neurons\n",
    "cnn.add(tf.keras.layers.Dense(128, activation='relu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output layer\n",
    "cnn.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "250/250 [==============================] - 44s 175ms/step - loss: 0.6732 - accuracy: 0.5786 - val_loss: 0.6207 - val_accuracy: 0.6655\n",
      "Epoch 2/50\n",
      "250/250 [==============================] - 45s 178ms/step - loss: 0.6126 - accuracy: 0.6643 - val_loss: 0.5825 - val_accuracy: 0.7035\n",
      "Epoch 3/50\n",
      "250/250 [==============================] - 44s 175ms/step - loss: 0.5595 - accuracy: 0.7116 - val_loss: 0.5384 - val_accuracy: 0.7295\n",
      "Epoch 4/50\n",
      "250/250 [==============================] - 45s 179ms/step - loss: 0.5394 - accuracy: 0.7232 - val_loss: 0.5654 - val_accuracy: 0.7120\n",
      "Epoch 5/50\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 0.5102 - accuracy: 0.7487 - val_loss: 0.5124 - val_accuracy: 0.7595\n",
      "Epoch 6/50\n",
      "250/250 [==============================] - 49s 195ms/step - loss: 0.4926 - accuracy: 0.7561 - val_loss: 0.4982 - val_accuracy: 0.7475\n",
      "Epoch 7/50\n",
      "250/250 [==============================] - 48s 190ms/step - loss: 0.4838 - accuracy: 0.7666 - val_loss: 0.4976 - val_accuracy: 0.7660\n",
      "Epoch 8/50\n",
      "250/250 [==============================] - 50s 199ms/step - loss: 0.4641 - accuracy: 0.7822 - val_loss: 0.4845 - val_accuracy: 0.7675\n",
      "Epoch 9/50\n",
      "250/250 [==============================] - 48s 194ms/step - loss: 0.4509 - accuracy: 0.7846 - val_loss: 0.4633 - val_accuracy: 0.7860\n",
      "Epoch 10/50\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 0.4423 - accuracy: 0.7903 - val_loss: 0.4963 - val_accuracy: 0.7565\n",
      "Epoch 11/50\n",
      "250/250 [==============================] - 51s 202ms/step - loss: 0.4349 - accuracy: 0.7944 - val_loss: 0.4745 - val_accuracy: 0.7830\n",
      "Epoch 12/50\n",
      "250/250 [==============================] - 49s 198ms/step - loss: 0.4168 - accuracy: 0.8071 - val_loss: 0.4899 - val_accuracy: 0.7830\n",
      "Epoch 13/50\n",
      "250/250 [==============================] - 50s 199ms/step - loss: 0.4126 - accuracy: 0.8116 - val_loss: 0.4714 - val_accuracy: 0.7775\n",
      "Epoch 14/50\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 0.3963 - accuracy: 0.8198 - val_loss: 0.4650 - val_accuracy: 0.7870\n",
      "Epoch 15/50\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 0.3936 - accuracy: 0.8170 - val_loss: 0.4593 - val_accuracy: 0.7805\n",
      "Epoch 16/50\n",
      "250/250 [==============================] - 49s 196ms/step - loss: 0.3822 - accuracy: 0.8253 - val_loss: 0.4543 - val_accuracy: 0.8005\n",
      "Epoch 17/50\n",
      "250/250 [==============================] - 50s 200ms/step - loss: 0.3688 - accuracy: 0.8340 - val_loss: 0.4882 - val_accuracy: 0.7865\n",
      "Epoch 18/50\n",
      "250/250 [==============================] - 48s 190ms/step - loss: 0.3664 - accuracy: 0.8342 - val_loss: 0.4630 - val_accuracy: 0.7960\n",
      "Epoch 19/50\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 0.3484 - accuracy: 0.8455 - val_loss: 0.4554 - val_accuracy: 0.7930\n",
      "Epoch 20/50\n",
      "250/250 [==============================] - 49s 198ms/step - loss: 0.3466 - accuracy: 0.8475 - val_loss: 0.4690 - val_accuracy: 0.7970\n",
      "Epoch 21/50\n",
      "250/250 [==============================] - 56s 226ms/step - loss: 0.3318 - accuracy: 0.8562 - val_loss: 0.4730 - val_accuracy: 0.7960\n",
      "Epoch 22/50\n",
      "250/250 [==============================] - 59s 234ms/step - loss: 0.3271 - accuracy: 0.8545 - val_loss: 0.4499 - val_accuracy: 0.8090\n",
      "Epoch 23/50\n",
      "250/250 [==============================] - 94s 377ms/step - loss: 0.3112 - accuracy: 0.8633 - val_loss: 0.4685 - val_accuracy: 0.8040\n",
      "Epoch 24/50\n",
      "250/250 [==============================] - 50s 199ms/step - loss: 0.3032 - accuracy: 0.8668 - val_loss: 0.4648 - val_accuracy: 0.8040\n",
      "Epoch 25/50\n",
      "250/250 [==============================] - 45s 182ms/step - loss: 0.2917 - accuracy: 0.8764 - val_loss: 0.5034 - val_accuracy: 0.8085\n",
      "Epoch 26/50\n",
      "250/250 [==============================] - 51s 205ms/step - loss: 0.2848 - accuracy: 0.8771 - val_loss: 0.4919 - val_accuracy: 0.7990\n",
      "Epoch 27/50\n",
      "250/250 [==============================] - 52s 208ms/step - loss: 0.2795 - accuracy: 0.8801 - val_loss: 0.4977 - val_accuracy: 0.7920\n",
      "Epoch 28/50\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 0.2615 - accuracy: 0.8895 - val_loss: 0.5033 - val_accuracy: 0.7995\n",
      "Epoch 29/50\n",
      "250/250 [==============================] - 75s 299ms/step - loss: 0.2573 - accuracy: 0.8878 - val_loss: 0.5473 - val_accuracy: 0.7975\n",
      "Epoch 30/50\n",
      "250/250 [==============================] - 58s 233ms/step - loss: 0.2472 - accuracy: 0.8944 - val_loss: 0.5146 - val_accuracy: 0.8025\n",
      "Epoch 31/50\n",
      "250/250 [==============================] - 59s 237ms/step - loss: 0.2417 - accuracy: 0.8992 - val_loss: 0.5030 - val_accuracy: 0.8030\n",
      "Epoch 32/50\n",
      "250/250 [==============================] - 57s 229ms/step - loss: 0.2329 - accuracy: 0.9078 - val_loss: 0.5395 - val_accuracy: 0.8000\n",
      "Epoch 33/50\n",
      "250/250 [==============================] - 50s 199ms/step - loss: 0.2408 - accuracy: 0.8969 - val_loss: 0.5353 - val_accuracy: 0.7855\n",
      "Epoch 34/50\n",
      "250/250 [==============================] - 49s 195ms/step - loss: 0.2266 - accuracy: 0.9065 - val_loss: 0.5040 - val_accuracy: 0.7940\n",
      "Epoch 35/50\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 0.2129 - accuracy: 0.9145 - val_loss: 0.5536 - val_accuracy: 0.7995\n",
      "Epoch 36/50\n",
      "250/250 [==============================] - 49s 195ms/step - loss: 0.2281 - accuracy: 0.9054 - val_loss: 0.5641 - val_accuracy: 0.8005\n",
      "Epoch 37/50\n",
      "250/250 [==============================] - 52s 209ms/step - loss: 0.1998 - accuracy: 0.9176 - val_loss: 0.5808 - val_accuracy: 0.7965\n",
      "Epoch 38/50\n",
      "250/250 [==============================] - 49s 195ms/step - loss: 0.1948 - accuracy: 0.9225 - val_loss: 0.5667 - val_accuracy: 0.8080\n",
      "Epoch 39/50\n",
      "250/250 [==============================] - 48s 194ms/step - loss: 0.1904 - accuracy: 0.9230 - val_loss: 0.5884 - val_accuracy: 0.7955\n",
      "Epoch 40/50\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 0.1814 - accuracy: 0.9280 - val_loss: 0.5552 - val_accuracy: 0.8130\n",
      "Epoch 41/50\n",
      "250/250 [==============================] - 48s 194ms/step - loss: 0.1799 - accuracy: 0.9271 - val_loss: 0.5568 - val_accuracy: 0.8135\n",
      "Epoch 42/50\n",
      "250/250 [==============================] - 49s 195ms/step - loss: 0.1697 - accuracy: 0.9312 - val_loss: 0.5908 - val_accuracy: 0.7970\n",
      "Epoch 43/50\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 0.1641 - accuracy: 0.9334 - val_loss: 0.6057 - val_accuracy: 0.7915\n",
      "Epoch 44/50\n",
      "250/250 [==============================] - 49s 195ms/step - loss: 0.1612 - accuracy: 0.9346 - val_loss: 0.6383 - val_accuracy: 0.8010\n",
      "Epoch 45/50\n",
      "250/250 [==============================] - 49s 195ms/step - loss: 0.1560 - accuracy: 0.9364 - val_loss: 0.6598 - val_accuracy: 0.7965\n",
      "Epoch 46/50\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 0.1487 - accuracy: 0.9400 - val_loss: 0.6319 - val_accuracy: 0.8045\n",
      "Epoch 47/50\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 0.1443 - accuracy: 0.9430 - val_loss: 0.6373 - val_accuracy: 0.8050\n",
      "Epoch 48/50\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 0.1455 - accuracy: 0.9430 - val_loss: 0.6333 - val_accuracy: 0.8005\n",
      "Epoch 49/50\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 0.1448 - accuracy: 0.9433 - val_loss: 0.6686 - val_accuracy: 0.8010\n",
      "Epoch 50/50\n",
      "250/250 [==============================] - 49s 198ms/step - loss: 0.1290 - accuracy: 0.9488 - val_loss: 0.6943 - val_accuracy: 0.7945\n"
     ]
    }
   ],
   "source": [
    "cnn.fit(x=training_set, validation_data= testing_set, epochs=50)\n",
    "cnn.save_weights('weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.load_weights('weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data/dataset/single_prediction/cat_or_dog_1.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-571d342f8da5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data/dataset/single_prediction/cat_or_dog_1.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    298\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minterpolation\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msupported\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m   \"\"\"\n\u001b[0;32m--> 300\u001b[0;31m   return image.load_img(path, grayscale=grayscale, color_mode=color_mode,\n\u001b[0m\u001b[1;32m    301\u001b[0m                         target_size=target_size, interpolation=interpolation)\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/keras_preprocessing/image/utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    111\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[1;32m    112\u001b[0m                           'The use of `load_img` requires PIL.')\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data/dataset/single_prediction/cat_or_dog_1.jpg'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('Data/dataset/single_prediction/cat_or_dog_1.jpg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = cnn.predict(test_image)\n",
    "training_set.class_indices\n",
    "if result[0][0] == 1:\n",
    "  prediction = 'dog'\n",
    "else:\n",
    "  prediction = 'cat'\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
